{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73bca264-b84a-4701-9d86-ab56d7c7ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/user5/Downloads/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528def18-c0ac-4e28-b2b1-f94218dc0036",
   "metadata": {},
   "source": [
    "Shuffling in Spark — In Short\n",
    "What it is → Moving data between partitions/nodes so related records end up together.\n",
    "\n",
    "Why it happens → Needed when:\n",
    "\n",
    "Grouping (groupBy, reduceByKey)\n",
    "\n",
    "Joining datasets\n",
    "\n",
    "Sorting\n",
    "\n",
    "Repartitioning\n",
    "\n",
    "\n",
    "Why it’s “bad” →\n",
    "\n",
    "Slow → network transfer between nodes.\n",
    "\n",
    "Costly → serialization, disk I/O if memory overflows.\n",
    "\n",
    "Unbalanced → can leave some partitions empty or overloaded.\n",
    "\n",
    "\n",
    "\n",
    "Mental model\n",
    "No shuffle → Data stays where it is → fast.\n",
    "\n",
    "Shuffle → Spark “mixes and sends” data across the cluster → slower.\n",
    "\n",
    "Use only when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f2295f-c831-4956-be71-7582c9d51b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/06 12:08:38 WARN Utils: Your hostname, datumlabs-Latitude-5420, resolves to a loopback address: 127.0.1.1; using 192.168.1.167 instead (on interface wlp0s20f3)\n",
      "25/08/06 12:08:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 12:08:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Create Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ShuffleDemo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68128b59-6408-4083-8ad7-a4bd3b6b9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of partitions: 3\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Create Sample Data\n",
    "\n",
    "# Sample sales data (store_id, sales_amount)\n",
    "data = [\n",
    "    (\"store1\", 100), (\"store2\", 50),\n",
    "    (\"store1\", 200), (\"store2\", 75),\n",
    "    (\"store3\", 60), (\"store3\", 90)\n",
    "]\n",
    "\n",
    "rdd = sc.parallelize(data, 3)  # force 3 partitions\n",
    "print(\"Initial number of partitions:\", rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d377ceac-a876-4a49-b9d2-76f06b1f491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original data distribution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 3) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: [('store1', 100), ('store2', 50)]\n",
      "Partition 1: [('store1', 200), ('store2', 75)]\n",
      "Partition 2: [('store3', 60), ('store3', 90)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "#Step 3: View Data by Partition\n",
    "\n",
    "def show_partitions(rdd, label):\n",
    "    print(f\"\\n{label}\")\n",
    "    for i, part in enumerate(rdd.glom().collect()):\n",
    "        print(f\"Partition {i}: {part}\")\n",
    "\n",
    "show_partitions(rdd, \"Original data distribution\")\n",
    "\n",
    "#enumerate() function is a Python tool that allows you to loop through an iterable—like a list, tuple, or string—and have access to both the index and the element itself.\n",
    "\n",
    "#glom()\n",
    "#Spark normally hides partition boundaries from you. we used glom to see our partitions\n",
    "\n",
    "\n",
    "\n",
    "#The 3 at the end → number of total tasks in this stage \n",
    "\n",
    "#The 0 before the plus → number of finished tasks.\n",
    "\n",
    "#The + 3 → number of tasks currently running.\n",
    "\n",
    "#these partitons are alos decided by roundrobin algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c82e09a8-c8e0-463c-9441-00ca7112285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Sales (no shuffle): 575\n",
      "\n",
      "After no-shuffle operation\n",
      "Partition 0: [('store1', 100), ('store2', 50)]\n",
      "Partition 1: [('store1', 200), ('store2', 75)]\n",
      "Partition 2: [('store3', 60), ('store3', 90)]\n"
     ]
    }
   ],
   "source": [
    "# Step 4: No Shuffle Example\n",
    "\n",
    "# No shuffle - just map and sum\n",
    "total_sales = rdd.map(lambda x: x[1]).sum()\n",
    "print(\"\\nTotal Sales (no shuffle):\", total_sales)\n",
    "\n",
    "# Check partitions again (should be unchanged)\n",
    "show_partitions(rdd, \"After no-shuffle operation\")\n",
    "\n",
    "# map -> for all k lie change kr deta he "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15074421-6df7-4bfd-9fd0-c3b946242a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales per store (with shuffle): [('store3', 150), ('store1', 300), ('store2', 125)]\n",
      "\n",
      "After shuffle (reduceByKey) distribution\n",
      "Partition 0: [('store3', 150)]\n",
      "Partition 1: [('store1', 300), ('store2', 125)]\n",
      "Partition 2: []\n"
     ]
    }
   ],
   "source": [
    "# Step 5: With Shuffle Example\n",
    "\n",
    "# With shuffle - reduceByKey\n",
    "sales_per_store = rdd.reduceByKey(lambda x, y: x + y)\n",
    "#reduceByKey() -> Combine values with the same key using a function.\n",
    "\n",
    "# Show per-store sales\n",
    "print(\"\\nSales per store (with shuffle):\", sales_per_store.collect())\n",
    "\n",
    "# Check new partition distribution after shuffle\n",
    "show_partitions(sales_per_store, \"After shuffle (reduceByKey) distribution\")\n",
    "\n",
    "\n",
    "#hash(\"store1\") % 3 → 1\n",
    "#hash(\"store2\") % 3 → 1\n",
    "#hash(\"store3\") % 3 → 0\n",
    "#Why Partition 2 is empty ?  Because none of your keys hashed to % 3 == 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70fc0b0b-0fbc-4c71-b494-8e654101184a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2AVH847.localdomain:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ShuffleDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7a56962d4440>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e51c81-83fc-49bb-b7d4-e436083ebde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
