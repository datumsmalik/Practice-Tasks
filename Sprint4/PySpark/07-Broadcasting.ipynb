{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea08f4e-bfbb-417f-ac83-e9105af8bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the PySpark environment variables\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/home/user5/Downloads/spark\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080649d1-ff1b-415d-991f-28d8b82b8409",
   "metadata": {},
   "source": [
    "Broadcasting in Spark \n",
    "\n",
    "\"Sending a small dataset to all worker nodes once so it can be used locally by tasks without shuffling\".\n",
    "\n",
    "Why it’s useful\n",
    "\"Avoids repeated sending of the same data to executors.\"\n",
    "\n",
    "Avoids shuffle join when joining big dataset + tiny dataset.\n",
    "\n",
    "When to use\n",
    "\"Small lookup tables or reference data used in joins, filters, or maps.\"\n",
    "\n",
    "When one dataset is much smaller than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea63c9c-4c69-4d39-be6d-0bf21cf5edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:09:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#step1 :Create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BroadcastDemo\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7935942b-0bd9-444e-af45-367368d7589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#step2: Create Big & Small DataFrames\n",
    "#We’ll pretend big_df is a large dataset and small_df is a small lookup table.\n",
    "\n",
    "# Big dataset: 1 million rows\n",
    "big_data = [(i, f\"val{i}\") for i in range(1, 1000001)]\n",
    "big_df = spark.createDataFrame(big_data, [\"id\", \"value\"])\n",
    "\n",
    "# Small lookup table: 5 rows\n",
    "small_data = [(1, \"A\"), (50, \"B\"), (100, \"C\"), (500, \"D\"), (1000, \"E\")]\n",
    "small_df = spark.createDataFrame(small_data, [\"id\", \"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16391e98-37ab-4abb-ac69-41084bada91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:04:45 WARN TaskSetManager: Stage 0 contains a task of very large size (2094 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+\n",
      "|  id|  value|category|\n",
      "+----+-------+--------+\n",
      "|  50|  val50|       B|\n",
      "|   1|   val1|       A|\n",
      "| 100| val100|       C|\n",
      "| 500| val500|       D|\n",
      "|1000|val1000|       E|\n",
      "+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step3 : Join Without Broadcasting (Shuffle Happens)\n",
    "\n",
    "joined_normal = big_df.join(small_df, \"id\")\n",
    "joined_normal.show(5)\n",
    "\n",
    "# You’ll see Shuffle Read / Shuffle Write in the join stage in spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e081d0-5e5b-472f-8392-4d3b748eac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2AVH847.localdomain:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BroadcastDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72c7002c40e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "330c25f3-350e-496d-97b0-6d3372290d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46fe04ca-b89e-4eda-a6a8-b33f9786efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 13:16:26 WARN TaskSetManager: Stage 8 contains a task of very large size (2094 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/06 13:16:26 WARN TaskSetManager: Stage 9 contains a task of very large size (2330 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/06 13:16:26 WARN TaskSetManager: Stage 10 contains a task of very large size (2330 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/06 13:16:26 WARN TaskSetManager: Stage 11 contains a task of very large size (2330 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/06 13:16:27 WARN TaskSetManager: Stage 12 contains a task of very large size (2330 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/08/06 13:16:27 WARN TaskSetManager: Stage 13 contains a task of very large size (2330 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+\n",
      "|  id|  value|category|\n",
      "+----+-------+--------+\n",
      "|   1|   val1|       A|\n",
      "|  50|  val50|       B|\n",
      "| 100| val100|       C|\n",
      "| 500| val500|       D|\n",
      "|1000|val1000|       E|\n",
      "+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step4 :Join With Broadcasting (No Shuffle for small_df)\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "joined_broadcast = big_df.join(broadcast(small_df), \"id\")\n",
    "joined_broadcast.show(5)\n",
    "\n",
    "#No shuffle stage for the join ap spark UI me view kr skte hain\n",
    "#small_df is sent once to all executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fc453f-124c-4fc0-8a29-7e3c51b295bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2AVH847.localdomain:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BroadcastDemo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x72c6cb7dd070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43cbd5-6089-4360-8cdc-55a7220f0770",
   "metadata": {},
   "source": [
    "Difference:::\n",
    "Normal Join (Shuffle Join)\n",
    "Both datasets are shuffled so matching keys go to the same partition.\n",
    "\n",
    "Slow for big datasets → lots of network traffic.\n",
    "\n",
    "Broadcast Join\n",
    "Small dataset is copied (broadcast) to all executors once.\n",
    "\n",
    "Big dataset is scanned locally → no shuffle for the small one.\n",
    "\n",
    "Much faster when one dataset is tiny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a351de5c-c6d1-48c1-86f8-859628f01927",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3908d385-0bb7-49fa-858f-d253445558b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
